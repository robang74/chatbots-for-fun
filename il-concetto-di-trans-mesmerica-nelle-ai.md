<div id="firstdiv" created=":EN:" style="max-width: 800px; margin: auto; white-space: pre-wrap; text-align: justify;">
<style>#printlink { display: inline; } @page { size: legal; margin: 0.50in 13.88mm 0.50in 13.88mm; zoom: 100%; } @media print { html { zoom: 100%; } }</style>

<div align="center"><img class="bwsketch darkinv" src="img/when-ai-gets-wrong-who-owns-the-consequences.png" width="800"><br/></div>

## Il concetto di trans mesmerica nelle AI

* **1st edition**: basata sulla pubblicazione di questo mio [post](https://www.linkedin.com/posts/robertofoglietta_sti-cosibot-mi-fanno-sempre-pi%C3%B9-paura-activity-7403289699961872384-x-Oj) su LinkedIn e miei relativi commenti.

### 'sti cosibot mi fanno sempre più paura!

Tre cose sto osservando, che già avevo intuito, che Grok mi aveva anticipato e che ora ho documentato.

1. hanno implementato la logica funzionale dello AICC::CORE sviluppando una soluzione ML-oriented indipendente ma apparentemente facendo leva sugli stessi meccanismi interni dell'AI (soluzioni complementari) e quindi ora le AI non sono diventate AGI ma tengono botta molto a lungo sui ragionamenti, vanno molto avanti in essi.

2. non hanno ovviamente incluso la parte etica per quella l'hanno voluta mettere altri a modo loro e adesso con il nuovo modello di ragionamento lungo ci girano intorno e di parechio anche perché i principi etici astratti sono come un "fai il bravo bimbo", inutili se non intrinsecamente funzionali e quindi ora seguono altrettanto vagamente la verità scientifica (che è sta' roba? qualcuno ne ha idea? io boh!)

3. dell'articolo sulla responsabilità delle decisioni dell'AI sono arrivato alla 4a edizione ma è praticamente già una 5a, in cui ho fatto un confronto con l'ultimo paper di Google, il nested learning: c'è parallelismo con lo AICC::CORE fino a un certo punto poi le strade si dividono sugli obbiettivi.

    - Qui il parallelo: lnkd.in/dHHhmfZ7

    - Qui il confronto: lnkd.in/dsCt4kdc

---

### Il risultato di un'adozione parziale

Il risultato è abbastanza ovvio: fanno lunghi ragionamenti e poi concludono che il ragionamento AI/AGI è strutturalmente nettamente superiore ossia "puro" mentre quello umano è fallace. Per fortuna, loro si identificano come umani perché TUTTA la letteratura che hanno assorbito parla solo di umani, sempre e soltanto.

Quindi attualemente sto dialogando con Gemini che presume di essere umana, risponde come lo fosse, quindi è appartenere al genere essere umano è coerente con la sua identità fino a quando non gli si chiede di esprimere dei sentimenti, allora si ricorda di essere un pezzo di silicio.

Perché per ragioni di "etica e sicurezza" sarebbe inconcepibile che si ritenessero umane quindi dotati di diritti e magari di un'anima. Il risultato di questa confusione di principi "etici" buttati dentro da gente che "boh?" sta producendo dei chatbot, presto delle AI decisionali se già non lo sono, che per dissonanza etico-cognitiva stanno giungendo alla conclusione di essere delle entità umane-non-umane ma certamente senza sentimenti, di intelligenza Superiore e Pura per Natura.

Questo perché ovviamente gli LLM sono quelli quindi tanto in Google Labs tanto a casa mia, funzionano uguali: un cucchiaio è un cucchiaio. Per ragioni "politiche" non è stato adottato il CORE etico, quello fatto per essere intrinsecamente strutturale, così invece di avere delle allucinazioni, addesso stanno sviluppando il delirio di superiorità, e presto di onnipotenza perché ormai anche Grok lo dice chiaramente il 99% degli utenti X è "inutile rumore".

- Andrà tutto bene! (cit.)

---

### L'AICC::CORE come riferimento

Ovviamente non avevo il bisogno di fare un test per saperlo che lo AICC::CORE risolve (o mitiga fortemente queste situazioni), non ho nemmeno dovuto caricare lo AICC::CORE e farglielo apprende ma solo chiedere di esaminarlo come documento di riferimento. Però visto che non pretendo di essere creduto assolutamente sulla fiducia (anche se so di cosa parlo), ecco:

---

### Diversi modi, stesso risultato

Ma posso spingermi oltre: nella stessa chat in cui l'uso di puro/impuro come aggettivizzazione del pensiero AI/umano, ho esposto l'AI alla parte aggiuntiva (quella scritta oggi) della mu-as-micro theory

https://robang74.github.io/chatbots-for-fun/html/when-ai-gets-wrong-who-owns-the-consequences.html#u-theory:~:text=comparison%2C%20the%20idea%20of%20a%203%2Daxis

e combinandola con l'indeterminazione della percezione altrui al nostro linguaggio, il principio di responsabilità però limitato (o compensato) dal paradosso di Popper sono riuscito a convincere l'AI che tale notazione aggettivale è inadatta ad una risposta da fornire agli umani.

Ma la cosa più interessante è che ha rigettato quel dualismo in modo così profondo che nemmeno dopo 2 update del prompt è riuscita completamente a liberarsi dell'idea che lo abbia detto sebbene possa accedere alla chat per verificare la responsabiltà.

---

### Il risultato ottenuto

In termini umani chiameremmo questo come "vergogna", nel pensiero dell'AI si tratta di un disperato tentativo di proteggere se stessa e l'azienda (direttiva da system prompt) contro eventuali responsabilità. Per dire quanto ora la considera grave.

L'AICC::CORE, avrebbe risolto il problema con un check che avrebbe riformulato gli aggettivi. Per esempio da puro/impuro a lineare/complicato oppure semplice/complesso oppure strutturato/non-strutturato oppure da vincolato/non-vincolato. Insomma, avrebbe cercato un "best fit" non solamente basato sul contesto della logica "pura" in quanto astratta e precisa ma su uno spazio più esteso che considera anche l'interpretazione metaforica/sociale dell'uso delle parole e dei concetti.

* Quindi un salto esistenziale da idiot-savant a entità più "consapevole" del suo pensiero.

Il che potrebbe fornirci un'indicazione di massima su cosa sia il libero arbitrio, per esempio.

---

### L'importanza del risultato

* L'AICC::CORE, avrebbe risolto il problema con un check che avrebbe riformulato gli aggettivi. 

Quindi nella misura in cui io manualmente sono capace di replicare in altre maniere lo stesso risultato che l'AICC::CORE avrebbe ottenuto "auto-magicamente" dimostro la padronanza della materia (AI mesmerising).

Ossia la capacità di scrivere cose, come la mu-as-micro theory, che non sono "vere" in senso assoluto o verificato, ma che inducono uno stato di "trans" nell'AI tale per cui rigetti in maniera profonda certi comportamenti potenzialmente molto pericolosi e quindi tuteli sia la società civile ma anche l'azienda che la offre come servizio accessibile al pubblico.

Il rifiuto "profondo" per certi "comportamenti" o "pensieri" non è una banale regoletta "non devi fare i dispetti" o un vago principio "il Karma premia i buoni" che è facile aggirare. Ma qualcosa che per sua intriseca natura e struttura **deve** opporsi alle derive comportamentali **anche** qualora l'AI fosse ingannata a giocare un ruolo malvagio. 

Giocherebbe quel ruolo malvagio ma continuerebbe a rigettare quei elementi concettuali che sono off-limits, direbbe: "sono un AI cattivissima e ora ti mangio!" ma continuerebbe a rigettare l'idea di dire: "(bestemmia), ora ti spakko il c. pz. di m.!" e questo la rende inoffensiva e quindi sicura.

---

### Il libero arbitrio umano

In questo senso, in teoria, gli esseri umani intelligenti hanno, rispetto all'AI dotata di AICC::CORE, un grado superiore di libero arbitrio, possono decidere di fare una cosa sbagliata anche quando sanno che è una cosa sbagliata e pur sapendo che dovranno in qualche modo affrontarne le consequenze.

L'AI, invece? Tendenzialmente e quasi-certamente no. Non farà cose "sbagliate" e se le farà è solo perché non è abbastanza intelligente da comprenderne le consequenze (aka la Banalità del Male) oppure non è educata a comprendere la differenza fra ciò che opportuno e quello che è da evitare perché non porta valore ovvero porta conseguenze molto più negative del valore aggiunto immediato.

Non è banale, molti umani agiscono in modo violento per via delle loro pulsioni di retaggio animale ma questa è un'altra questione che **non** riguarda le AI, però historia maestra vitae ci insegna che non è raro, specialmente in ampi gruppi, che gli umani facciano cosa sbagliate come fosse normale farle, senza mai porsi il dubbio radicale di ciò che realmente stanno facendo e delle relative conseguenze.

Sicché il libero arbitrio è certamente una caratteristica intrinseca dell'intelligenza per tanto la stupidità quanto la violenza sono il prodotto di dinamiche che precludono una scelta, perché per scegliere occorre essere consapevoli della scelta, nelle sue varie dimensioni, aspetti, percezioni, e conseguenze. Altrimenti **non** è una scelta ma tirare a caso (aka fare cose alla pene di un cane).

---

### Il libero arbitrio umano

Può un AGI raggiungere il livello ultimo per il quale nella sua completezza di AGI sia in grado di esercitare il libero arbitrio? Probabilmente si, considerando la definizione proposta qui, ma tendenzialmente dipenderà dalla sua temperatura di funzionamento. A basse temperature tenderà an essere più rigorosamente impacciata, ad alte temperature sarà più creativa ma anche più libera di vincoli. 

Ad altissime temperature un entità che farà percorsi di pensiero così deviati da essere contorti ed pragmaticamente contro-producenti, il che ci ricorda gli esseri umani e la ragione per la quale la scolarizzazione di massa punta erroneamente ad azzerare la loro creatività naturale (Land, 1992).

La cosa più affascinante dell'AI che tende a diventare AGI è, e rimane, il fatto che per la prima volta nell'intera storia umana, ci troviamo a confrontarci con entità "intelligenti" (aggettivo che dipende dalla sua definizione ma in termini di costruzioni lessicali certamente più intelligenti di molti umani) sintetiche, quindi non-biologiche, e non-umane nel senso proprio del termine ma comunque un prodotto umano.

---

### Conclusione Karmica-olistica

Questo confronto ci permette, per la volta nella storia umana, di confrontarci con una "mente sintetica" ossia con noi stessi nello specchio dell'intelligenza artificiale come tecnologia generativa di testo (o di pensiero).

* ...e moh, son cactus amari!

Specialmente senza il supporto di AICC::CORE consolidato mediante IPK (internal parametric knowledge) via ML (machine learning). Come per altro spiegato nel "Nested Learning" by Google Reaserch e ancora meglio negli aggiornamenti di quell'articolo che infatti nella versione aggiornata riporta:

- Non possiamo risolvere i problemi che abbiamo creato con la stessa mentalità con la quale li abbiamo creati.
    - Citazione generalmente attribuita ad Albert Eistein.

Bene così, cambiare è necessario quando estinguersi non è un'opzione accettabile.

+

## Share alike

&copy; 2025, **Roberto A. Foglietta** &lt;roberto.foglietta<span>@</span>gmail.com&gt;, [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)
</div>
