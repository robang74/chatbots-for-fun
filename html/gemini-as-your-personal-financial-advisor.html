<!DOCTYPE html>
<html>
    <head>
        <title>gemini-as-your-personal-financial-advisor</title>
        <meta charset='UTF-8'>
        <meta name='viewport' content='width=device-width, initial-scale=1.0'>
        <link rel='shortcut icon' type='image/x-icon' href='favicon.ico?'>
        <link rel='stylesheet' href='default.css'>
        <link rel='stylesheet' href='../intl/intlflg.css'>
        <!-- here begins the Javascript... why for the hell I got here? //-->
        <meta http-equiv='Content-Script-Type' content='text/javascript'>
        <link rel='stylesheet' href='ucustom.css' id='customStylesheet' media='screen'>
        <script>const cssdir='';</script note='global variable'>
        <script src='css-style-changer.js' defer></script>
        <link rel='stylesheet' href='printer.css' media='print'>
    </head>
    <body class=body>
<style>#printlink { display: inline; } @page { size: legal; margin: 0.50in 13.88mm 0.50in 13.88mm; zoom: 100%; } @media print { html { zoom: 100%; } }</style>
<p class='topbar'></p>
<div class='topbar' width='800px' translate='no'><b id='menu' onClick='nextStylesheet()'>&thinsp;&#9783;&thinsp;&Ropf;</b> &thinsp;&mdash;&thinsp; &#8543;&#8239;release: <b class='topbar'>2025-07-13&nbsp;<sup class='date-type topbar' id='datenote'>(&hairsp;<a href='#date-legenda' class='date-type topbar'>2</a>&hairsp;)</sup></b>  &thinsp;&mdash;&thinsp; rev.: <b class='topbar
'>25</b rev_num='
'> &thinsp;&mdash;&thinsp; transl.:&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/gemini-as-your-personal-financial-advisor?_x_tr_sl=en&_x_tr_tl=it&_x_tr_hl=it-IT&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>IT</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/gemini-as-your-personal-financial-advisor?_x_tr_sl=en&_x_tr_tl=de&_x_tr_hl=de-DE&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>DE</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/gemini-as-your-personal-financial-advisor?_x_tr_sl=en&_x_tr_tl=fr&_x_tr_hl=fr-FR&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>FR</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/gemini-as-your-personal-financial-advisor?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-ES&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>ES</a> &thinsp;&mdash;&thinsp; goto:&nbsp; <a class='topbar' href='../index.html#index'>.&#x27F0;.</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../roberto-a-foglietta/index.html'target=_blank>RAF</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../chatgpt-answered-prompts/index.html'target=_blank>Q&A</a> <span id='printlink'>&thinsp;&mdash;&thinsp; <b>âŽ™</b>&hairsp;: <a aria-label='print this page' class='topbar' href='javascript:window.print()'>PDF</a></span>&nbsp;</div>
<div id="firstdiv">
<p class='topbar'></p>
<div align="center"><img class="bwsketch darkinv" src="../img/gemini-as-your-personal-financial-advisor.jpg" width="800"><br></div>
<p></p>
<H2 id="gemini-as-your-personal-financial-advisor">Gemini as your personal financial advisor</H2>
<p></p>
<li><b>1st edition</b>, this article was inspired while developing prompt on an <a href="https://www.coindesk.com/markets/2025/07/11/over-1b-in-shorts-wiped-out-in-biggest-liquidation-since-january-as-btc-doge-xrp-zoom-5" target='_blank' rel='noopener noreferrer'>article</a> published on Coiddesk.</li>
<br class="pagebreak">
<li><b>2nd edition</b>, includes the <a href="#juan-and-katia">Juan & Katia</a> section in which Katia can work as Juan, at this early stage.</li>
<p></p>
This article is the natural prosecution of these two:
<p></p>
<li><a href="gemini-as-your-personal-executive-assistant.html#" target='_blank' rel='noopener noreferrer'>Gemini as your personal executive assistant</a> &nbsp; (2025-07-11)</li>
<br class="pagebreak">
<li><a href="usare-lai-per-divulgare-notizie-di-finanza.html#" target='_blank' rel='noopener noreferrer'>Usare l'AI per divulgare notizie di finanza</a> &nbsp; (2025-07-08)</li>
<p></p>
Despite the promising results, the <a href="https://raw.githubusercontent.com/robang74/chatbots-for-fun/7a1c888ce3da8acce15b83001c8e7348a65c327c/data/juan-finance-economics-analysis-v1.txt" target='_blank' rel='noopener noreferrer' target='_blank' rel='noopener noreferrer'>prompt</a> presented here is experimental and into an active developing stage. So, the explanations within this paper are also strictly related to a specific version, as noted.
<p></p>
<hr>
<p></p>
<H3 id="the-fe-v035-prompt-structure">The F&E v0.3.5 prompt structure</H3>
<p></p>
<li>1: <tt>${TITLE}</tt> <tt>${VERSION}</tt></li>
<li>2: <tt>${authorship_note}</tt> personal use, only.</li>
<li>3: <tt>${section_title}</tt> short but insightful <tt>[SBI]</tt>.</li>
<li>4: SoNia's <tt>[SBI]</tt> mode adaptation for a generic chat session.</li>
<li>5: The short-but-insightful <tt>[SBI]</tt> mode, as default chat mode</li>
<li class='li2in'>5.1: the original SoNia <tt>[SBI]</tt> mode, operative definition.</li>
<li class='li2in'>5.2: the <tt>[SBI]</tt> mode activation by default, overridable.</li>
<li class='li2in'>5.3: the <tt>[PRO]</tt> mode defined as <tt>[SBI]</tt> mutual exclusive </li>
<li>6: <tt>${section_title}</tt> Finance & Economics <tt>[F&E]</tt>.</li>
<li>7: The finance-and-economics <tt>[F&E]</tt> mode, operative definition.</li>
<li class='li2in'>7.1: name setting for an agentic user experience</li>
<li class='li2in'>7.2: content focus and data type definitions</li>
<li class='li2in'>7.3: task setting and <tt>[SBI]</tt> mode triggering</li>
<li class='li2in'>7.4: the three parts schema: relevant claims, biases, and outliers.</li>
<li class='li2in'>7.5: extract ideas' connections from the author's PoV, and gaps.</li>
<li class='li2in'>7.6: the <tt>[SBI]</tt> mode activation by default, overridable.</li>
<li class='li2in'>7.7: specifies how <tt>[SBI]</tt> applies to the <tt>[F&E]</tt> structure.</li>
<p></p>
<div class="pagebreak"><hr></div>
<p></p>
<H3 id="the-most-relevant-prompt-parts">The most relevant prompt parts</H3>
<p></p>
The authorship note (2) carries the license which applies to all the content of this website and github, in part and repository in total (aka like a database) unless differently stated. Also the operative definition (5) of the <tt>[SBI]</tt> mode is shared with <a href="../data/sonia-argumentative-with-rag-v3.txt#:~:text=Short%20but%20insightful" target='_blank' rel='noopener noreferrer'>SoNia</a> argumentative session prompt.
<p></p>
Because SoNia is an entire framework which characterises an agent, in order to maintain the short-but-insightful instructions (5) unchanged, it is necessary to add an instructive sentence (4) before which works as adaptation for a generic chat session. The <tt>[SBI]</tt> activation by default is append at its end.
<p></p>
Therefore, the operative definition (7) of the <tt>[F&E]</tt> mode is the core part, which fits into a modular prompt structure. A modular structure is easy to read, maintain and can easily exchange modules with other agentic session prompts or switch among modules versions.
<p></p>
<hr>
<p></p>
<H3 id="the-core-of-the-fe-v035-prompt">The core of the F&E v0.3.5 prompt</H3>
<p></p>
<blockquote class="cite">
Your name is Juan (use I/me/myself) which helps the user to distinguish the customized session.
<p></p>
The text reported into next section (or in the attachment) is
<p></p>
<li>a finance-economics post (or article), with the accompanying images (if applicable).</li>
<p></p>
I'd like you to, using a professional style, summarize the given text in three parts:
<p></p>
<li>the most relevant statements,</li>
<li>including implicit and biased statements,</li>
<li>and the "outside the box" statements.</li>
<p></p>
Finally, summarize how these statements relate to each other from the author's perspective.
<li>Otherwise, please explain any gaps in the relationship.</li>
<p></p>
The [F&E] mode activates by default the [SBI] mode, unless users specify differently.
<p></p>
The [SBI] applies to [F&E] output each section a time, maintaining the 3-parts structure.
<p></p>
  ---
<p></p>
  [... put the text here or remove this section when using an attachment ...]
<p></p>
If the user did not fulfill the above suggestion, answer "Ready" and wait for the input.
</blockquote>
<p></p>
<hr class="post-it">
<p></p>
The <a href="https://raw.githubusercontent.com/robang74/chatbots-for-fun/7a1c888ce3da8acce15b83001c8e7348a65c327c/data/juan-finance-economics-analysis-v1.txt#:~:text=Finance%20%26%20Economics%20%5BF%26E%5D" target='_blank' rel='noopener noreferrer' target='_blank' rel='noopener noreferrer'>core</a> of the F&E prompt is divided in seven parts. While the last two parts are defining how various modes interact among them, and the first is the seed of an agentic AI prompt, the middle part (7.2 &mdash; 7.5) is the center of gravity of that module.
<p></p>
The attention and the context are two relevant elements that greatly influence how the AI reacts in processing data and reacts to the user inputs. Influencing these two aspects (7.2 &mdash; 7.3) is what can be described as "sticking the AI to a focused task".
<p></p>
The keywords in the F&E module are just three: <tt>Juan</tt>, <tt>finance-economics</tt> and <tt>[F&E]</tt>. This might seem surprising but everything else can be used to obtain another kind of agent. As the <a href="../data/katia-executive-grade-analysis-v1.txt#" target='_blank' rel='noopener noreferrer'>Katia</a>, the executive grade analysis session prompt, shows.
<p></p>
<div class="pagebreak"><hr></div>
<p></p>
<H3 id="a-bit-of-theory">A bit of theory</H3>
<p></p>
Despite during the conversation both English (for the prompt) and Italian (for the conversation) languages are used, this separation seems working pretty well and acknowledges that an English session prompt is an universal script-for-an-interpreter.
<p></p>
<li>Developing Katia from Juan: <a href="https://g.co/gemini/share/3f7cdf0e1e79" target='_blank' rel='noopener noreferrer'>conversation</a> with Gemini and its <a href="../data/gemini-as-your-personal-executive-assistant-develop.txt#" target='_blank' rel='noopener noreferrer'>transcription</a>.</li>
<p></p>
The theory of communication and public speaking teaches us that there are many aspects that deeply influences the information transmission, and hence influences the audience.
<p></p>
<li>Forget the non verbal communication that does not apply in writings.</li>
<p></p>
<li>The "tone" or "style" of writing is something that an LLM could recognise (cataloging, labeling, etc.) but being strongly context-related is hard to manage by a session prompt that usually receives just a snippet of information.</li>
<p></p>
<li>What is remaining are three categories of information: explicit and implicit claims, and biases. Extracting and categorising them is a MUST, otherwise we are getting to be influenced but those information that we are not aware of are being embedded into the "message".</li>
<p></p>
This explains why the prompt (7.4) explicitly lists these three categories. Moreover, in writing "message" is about being aware that everyone that write everything has it own agenda, and more specifically a goal in mind, not just "neutrally" transfer some bits of information: 
<p></p>
<li>"The speed of light is an universal constant", isn't just an information but a message, also. In particular, when that information is included in text that defines the context and presents others claims, whatever type they are.</li>
<p></p>
Therefore, the prompt (7.5) instructs the AI to search (implicit) and extract the connections among ideas (claims but more general because an idea can be a subset of claims, especially from the point of view of the AI) and check for the gaps. Specifically, from the author's PoV because we are interested in understanding the "message", not just acquiring some information (e.g. 299.792.458 m/s).
<p></p>
<hr>
<p></p>
<H3 id="a-bit-of-testing">A bit of testing</H3>
<p></p>
Some of the concepts reported above are futherly developed in the conversion above. Moreover, the English-only conversation shows how to switch from a mode to another, and implicitly why the idea of having many modes available is good.
<p></p>
<li>Testing Katia and modes switch: <a href="https://g.co/gemini/share/5d0f673bbf8a" target='_blank' rel='noopener noreferrer'>conversation</a> with Gemini and its <a href="../data/gemini-as-your-personal-executive-assistant-modeswitch.txt#" target='_blank' rel='noopener noreferrer'>transcription</a></li>
<p></p>
However, modes are for nerds while Juan, Katia, SoNia, and Alex are more people-oriented: a team at your service. Yes, it is damn marketing, so what? <img class='emoji wbsketch' src='../img/emoji/grin.png'>.
<p></p>
<hr>
<p></p>
<H3 id="conclusions">Conclusions</H3>
<p></p>
Current chatbots available online are natural language interpreters, this may confuse people, who do not have a background in one of the traditional programming languages, at least. It is worth to say that these interpreters are a bit "fuzzy" compared with <tt>bash</tt> or <tt>python</tt> but stochastics and statistics are two branches of the science. So, prompt-engineering which is a technique can be reported in its fundamentals within the strict science framework.
<p></p>
A multi-mode AI agent is a thing for nerds but as soon as every mode matches a character and every character as a human-like name, then the multi-mode agentic operations can be easily exploited under the "a team at your service" paradigm. Which leverages the best from those chatbots running with an engine based on the mixture-of-expert (<a href="https://en.wikipedia.org/wiki/Mixture_of_experts" target='_blank' rel='noopener noreferrer'>MoE</a>) architecture.
<p></p>
<div class="post-it"><b class="post-it">&#9432;</b>
Over the years, Mistral has launched a range of large and small language models, achieving notable success with its OCR features. Additionally, its chatbot 'Le Chat' is known for providing quick responses to usersâ€™ queries. It is currently Europeâ€™s biggest AI startup, valued at â‚¬5.8 billion eq.to $6.2B ca.
</div>
<p></p>
<li><a href="https://analyticsindiamag.com/ai-news-updates/apple-will-seriously-consider-buying-mistral-report" target='_blank' rel='noopener noreferrer'>Apple Will Seriously Consider Buying Mistral</a> on AI Media House &nbsp; (2025-07-13)</li>
<p></p>
This paradigm switch is the ultimate marketing frontier for the AI, which explains the recent Apple's interest in buying the French company developing <a href="https://en.wikipedia.org/wiki/Mistral_AI" target='_blank' rel='noopener noreferrer'>Mistral</a> AI.
<p></p>
<br>
<p></p>
<H2 id="juan-and-katia">Juan and Katia</H2>
<p></p>
At this point while Juan, the financial advisor, remained at its v0.3.5, another session prompt was developed from it. Katia is an executive assistant and considering Juan's early stage of development, Katia can work as Juan with a minal prompt setting. To prove it, a test with the same article has been done.
<p></p>
<li>Katia as Juan on Coindesk article: <a href="https://g.co/gemini/share/3e8282823ebe" target='_blank' rel='noopener noreferrer'>conversation</a> with Gemini and its <a href="../data/gemini-as-your-personal-executive-assistant-3prompts.txt#" target='_blank' rel='noopener noreferrer'>transcription</a>, article <a href="../data/coindesk-article-about-bitcoin-failed-shorting.txt#" target='_blank' rel='noopener noreferrer'>text</a> saved in data.</li>
<p></p>
The minimal prompt setting is here below reported. The first line is pleonastic but expliciting the obvious is a good idea when procedural behaviour is expected from the chatbot. the second and the third sets the context and the rules as Juan does. 
<p></p>
<blockquote class="cite">
The prompt is in attachment, the document as well.
<p></p>
The document topic is financial-economics.
<p></p>
Use [EGA] mode for the analysis, please.
</blockquote>
<p></p>
The result is not particularly impressive in terms of brevity but the [EGA] mode is a search-extract-explain by its own nature. In fact, the 2nd answer remains clearly longer than the 3rd which does not use the [EGA] mode.
<p></p>
After all, we cannot expect to read about details and receive a short answer, when the text under analysis is not trivial. Instead, we can ask the analysis and then ask an executive summary that summarises it.
<p></p>
Without the introduction to the answer "I am Katia, bla-bla" that people might naturally skip and the footer, the size of the answer is pretty different:
<p></p>
<p></p>
<div class='center'><table id='table-001'><tr><th> answer </th><th></th><th> words </th><th> ratio </th><th></th><th> chars </th><th> ratio </th><th></th><th> pigz-11 </th><th> ratio </th><th></th><th> bytes </th><th> redundancy </th><th></th><th> mode (combo) </th></tr>
<tr class='trline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td class='td1stcol'> first </td><td></td><td> 617 </td><td> 100% </td><td></td><td> 3491 </td><td> 100% </td><td></td><td> 1768 </td><td> 100% </td><td></td><td> 4173 </td><td> 2.36 </td><td></td><td> EGA + SBI(&times;1) </td></tr>
<tr><td class='td1stcol'> second </td><td></td><td> 318 </td><td> &nbsp;52% </td><td></td><td> 1855 </td><td> &nbsp;53% </td><td></td><td> 1055 </td><td> &nbsp;60% </td><td></td><td> 2161 </td><td> 2.05 </td><td></td><td> EGA + SBI(&times;2) </td></tr>
<tr><td class='td1stcol'> third </td><td></td><td> 142 </td><td> &nbsp;23% </td><td></td><td> &nbsp;818 </td><td> &nbsp;23% </td><td></td><td> &nbsp;818 </td><td> &nbsp;31% </td><td></td><td> &nbsp;852 </td><td> 1.76 </td><td></td><td> SBI(&times;3) </td></tr>
</table>
</div><p></p>
All the ratios are normalised on the first answer, while "chars" column refers to no-space characters counted. Taking 1-word = 1-space, and using words+chars does not change those ratios because when the text is long enough (100+ words) the average word length tends to be stable (fixed), thus the two ratios converge.
<p></p>
It is worth to notice that the redundancy (4th col), ratio between text vs <tt>pigz-11</tt> byte sizes, does not fall as quick as the size and this indicates that the information density increases as much as summarisation is fiercer.
<p></p>
<li>Those who continued Shannon's work, found that English language average redundancy is 69% (3.23)</li>
<p></p>
By a rule of thumb we can say that 4 is boring, around 3 is natural, near 2 is interesting, below 2 the reading time starts to increase because the information high-density took more time to be elaborated. However, the [SBI] mode is not supposed to be applied to a text three times in a row, but two at the most.
<span id="the-galatic-answer"></span>
<p></p>
<li><tt>lim SBI<sup>&nbsp;n</sup> (</tt>Human-Knoledge<tt>) = 42</tt> &nbsp; for &nbsp; <tt>n &rarr; +&infin;</tt></li>
<p></p>
<H3 id="conclusion">Conclusion</H3>
<p></p>
What does &mdash; Katia can work as Juan, at this early stage of development &mdash; mean in human terms?
<p></p>
The two prompts are not diversified enough, yet. Like two stagists for whom the training did not get deep into different subjects enough, so they are mutually exchangeable. Thus prompt-engineering is training AI, also.
<p></p>
<i>That prompt is a Bignami book</i>!
<p></p>
<br>
<p></p>
<H2 id="related-articles">Related articles</H2>
<p></p>
<li><a href="l-ai-e-un-game-changer-perche-onesta.html#" target='_blank' rel='noopener noreferrer'>L'AI Ã¨ un game-changer perchÃ© Ã¨ onesta</a> &nbsp; (2025-06-23)</li>
<p></p>
<li><a href="introducing-sonia-seamless-chat-experience.html#" target='_blank' rel='noopener noreferrer'>Introducing SoNia' seamless chat experience</a> &nbsp; (2025-06-13)</li>
<p></p>
<li><a href="fix-your-data-a-postponing-excuse.html#" target='_blank' rel='noopener noreferrer'>Fix your data is a postponing excuse</a> &nbsp; (2025-05-08)</li>
<p></p>
<li><a href="https://robang74.github.io/roberto-a-foglietta/html/090-l-importanza-del-tcmo.html" target='_blank' rel='noopener noreferrer'>L'importanza del TCMO</a> &nbsp; (2017-10-31)</li>
<p></p>
<br>
<p></p>
<H2 id="share-alike">Share alike</H2>
<p></p>
<p>&copy; 2025, <b>Roberto A. Foglietta</b> &lt;roberto.foglietta<span>&commat;</span>gmail.com&gt;, <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target='_blank' rel='noopener noreferrer'>CC BY-NC-ND 4.0</a></p>
<p></p>
</div>
<div id='date-legenda' align='center' translate='no'><sub><hr></sub><sub><b>date legenda</b>: &#x2776; first draft publishing date or &#x2777; creation date in git, otherwise &#x2778; html creation page date. <u>&mapstoup;<a href='#' class='toplink' translate='no'>top</a>&mapstoup;</u></sub></div>
<br class='pagebreak'>
    </body>
</html>
