<!DOCTYPE html>
<html>
    <head>
        <title>how-to-use-katia-for-educating-katia</title>
        <meta charset='UTF-8'>
        <meta name='viewport' content='width=device-width, initial-scale=1.0'>
        <link rel='shortcut icon' type='image/x-icon' href='favicon.ico?'>
        <link rel='stylesheet' href='default.css'>
        <link rel='stylesheet' href='../intl/intlflg.css'>
        <!-- here begins the Javascript... why for the hell I got here? //-->
        <meta http-equiv='Content-Script-Type' content='text/javascript'>
        <link rel='stylesheet' href='ucustom.css' id='customStylesheet' media='screen'>
        <script>const cssdir='';</script note='global variable'>
        <script src='css-style-changer.js' defer></script>
        <link rel='stylesheet' href='printer.css' media='print'>
    </head>
    <body class=body>
<style>#printlink { display: inline; } @page { size: legal; margin: 0.50in 13.88mm 0.50in 13.88mm; zoom: 100%; } @media print { html { zoom: 100%; } }</style>
<p class='topbar'></p>
<div class='topbar' width='800px' translate='no'><b id='menu' onClick='nextStylesheet()'>&thinsp;&#9783;&thinsp;&Ropf;</b> &thinsp;&mdash;&thinsp; &#8543;&#8239;release: <b class='topbar'>2025-10-28&nbsp;<sup class='date-type topbar' id='datenote'>(&hairsp;<a href='#date-legenda' class='date-type topbar'>2</a>&hairsp;)</sup></b>  &thinsp;&mdash;&thinsp; rev.: <b class='topbar
'>16</b rev_num='
'> &thinsp;&mdash;&thinsp; transl.:&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/how-to-use-katia-for-educating-katia?_x_tr_sl=it&_x_tr_tl=en&_x_tr_hl=en-EN&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>EN</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/how-to-use-katia-for-educating-katia?_x_tr_sl=it&_x_tr_tl=de&_x_tr_hl=de-DE&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>DE</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/how-to-use-katia-for-educating-katia?_x_tr_sl=it&_x_tr_tl=fr&_x_tr_hl=fr-FR&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>FR</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='https://robang74-github-io.translate.goog/chatbots-for-fun/html/how-to-use-katia-for-educating-katia?_x_tr_sl=it&_x_tr_tl=es&_x_tr_hl=es-ES&_x_tr_pto=wapp' target='_blank' rel='noopener noreferrer'>ES</a> &thinsp;&mdash;&thinsp; goto:&nbsp; <a class='topbar' href='../index.html#index'>.&#x27F0;.</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../roberto-a-foglietta/index.html'target=_blank>RAF</a> &nbsp;<b>&middot;</b>&nbsp; <a class='topbar' href='../../chatgpt-answered-prompts/index.html'target=_blank>Q&A</a> <span id='printlink'>&thinsp;&mdash;&thinsp; <b>⎙</b>&hairsp;: <a aria-label='print this page' class='topbar' href='javascript:window.print()'>PDF</a></span>&nbsp;</div>
<div id="firstdiv">
<p class='topbar'></p>
<div align="center"><img class="bwsketch darkinv" src="../img/how-to-use-katia-for-educating-katia.jpg" width="800"><br/></div>
<p></p>
<H2 id="how-to-use-katia-for-educating-katia">How to use Katia for educating Katia</H2>
<p></p>
<li><b>1st edition</b>, explains how the <a href="../data/katia-executive-grade-analysis-v1.md#" target='_blank' rel='noopener noreferrer'>Katia framework</a> can be used for educating Katia AI agent, by a human.</li>
<p></p>
<li><b>2nd edition</b>, presents the <a href="#methodology">methodology</a> behind an use-case of a freely-available chatbot prepared and fine-tuned for efficiently performing as an AI agent over a informative corpus stored into an advanced cognitive retrieval virtual RAG-for-concepts system. Because retrieving by keywords is for a relational database, retrieving by patterns is a RAG like many others while retrieving information for concepts (logos) is how the human mind works. That's why education in AI terms makes such a sensitive difference.</li>
<p></p>
<hr class="post-it">
<p></p>
In the previous <a href="how-to-use-katia-for-improving-katia.html#" target='_blank' rel='noopener noreferrer'>article</a>, the Katia framework has been used to improve itself through an iterative process with a human operator (the author). The improving process was focused on the Human Knowledge and Opinions <tt>[HKO]</tt> module because it was the most sensitive one.
<p></p>
There is a sensitive difference between "improving" a framework, able to characterise an AI model as a session prompt, which is a process similar in some aspects to a software developing process and "educating" an AI agent. Despite, in general terms, both are intended to change how the AI model elaborates the information.
<p></p>
<hr>
<p></p>
<H3 id="a-necessary-premise">A necessary premise</H3>
<p></p>
Once upon a time neural networks were pure bayesian algorithms for pattern recognition. Introducing noise in the training data set was not a "features" rather than unavoidable "bug": some data sets were "impure" and such impurities were slowing down the training process. Injecting a relatively small noise during the learning process allowed the training process to be much more performant both in terms of speed and abstraction. Well, for sake of completeness, the "relatively small" implies that data should be normalized, and this greatly increased the speed in training rather than white noise itself.
<p></p>
Once trained in a neural network, it remains a pure Bayesian algorithm as long as discrete math is precise. Running with exact precise discrete math, a neural network model remains a deterministic algorithm that can reproduce the same output from the same input. Although determinism is a great feature, it can lead to very awful outcoming when it is used in <a href="la-banalita-del-male-nel-determinismo-ai.html#" target='_blank' rel='noopener noreferrer'>extreme</a> contexts in which the human instinct and responsibility are sanity boundaries.
<p></p>
Running with exact precise discrete math, a neural network model does not perform great. Much better is injecting a relatively small white noise that allows the NN to explore a wider range of solutions without being stuck in the first weak minimum that it can encounter along the quest of finding the best local minimum. By a certain extension these NN models are still Bayesian in principle but not anymore deterministic. Yet, "stupid" in human terms.
<p></p>
Current LLM available on-line are running with optimisation on hardware and software which introduce errors in digital calculations. This is <b>not</b> equivalent to adding a white noise to some NN layers. It is a completely different story because calculations errors compound (&hairsp;<i>chaotic divergence</i>&hairsp;) and they do so in such an unpredictable way that shifts a NN from a Bayesian model into the theory of chaos. This <b>IS</b> the novelty that brings GPT online as the first publicly mass-available chatbot.
<p></p>
It is a kind of magic that "intelligence" emerges as a meta-feature from chaos into an organised system.
<p></p>
<hr>
<p></p>
<H3 id="improving-vs-educating-p1">Improving vs Educating, p.1</H3>
<p></p>
About improving an AI model, there is many declination of this term and in particular, many subsystems involved: the bare hardware (GPU vs TPU), the cloud hardware infrastructure, the networking and load balancing, the containerisation (or/vs virtualisation), the LLM in terms of Bayesian model, the implementation of the model, the dataset size and quality, the training process, the fine-tuning, etc.
<p></p>
Fine-tuning is the process of taking a general AI model and adapting it to a specific, well-defined task. While LLM are designed to possibly fulfill a task chosen among many of various nature, in terms of process automatisation, it is an overkilling and counterproductive approach. That’s why Medium and Small Language Models (MLM, SLM) exist in the first place..
<p></p>
After all, we do not need an AI model that can deal with N languages and chat about philosophy when our core business is converting tables into indented lists. A small model fine tuned for that task is the perfect solution. However, in terms of training the fine-tuning is a quite expensive process. Therefore, the entry barrier is quite high, unless the fine-tuning is providing a set of directives (procedural schema) which are precise but simple enough to be executed.
<p></p>
Why instructing and fine-tuning can be equivalent? The foundation in terms of theory is solid and it is conventionally known as "learning by context". Where "context" in this case means by information provided at run time instead of at training time. You might say: "like a Python interpreter but in English". Not exactly!
<p></p>
An interpreter only executes, while even a SLM can learn from the context. It interpreters the procedure but also learns in terms of generalisation and during execution among many runs or sessions, shows slightly different behaviour which can be corrected or supported by a quality-check feedback system (a human).
<p></p>
The idea of Katia self-improving itself, is about the author of Katia (me) that acts like a quality-check feedback system. A trainer who prompts the AI model and considers the output transfer variations into the Katia framework in such a way a piece of information learnt or emerged into a specific session would be available in all the future sessions based on the Katia framework.
<p></p>
Therefore "improving" as written in the previous article is about fine-tuning and taking notes.
<p></p>
<hr>
<p></p>
<H3 id="improving-vs-educating-p2">Improving vs Educating, p.2</H3>
<p></p>
Educating is totally another story, and it is not fine-tuning but providing a generalist approach yet sounding grounded in solid principles. Each principle contributes to completeness, but also introduces incoherence or ambiguity.. In extreme terms, Gödel's incompleteness theorems where N principles are not enough and N+1 are too much.
<p></p>
Once a certain set of principles has been chosen, they should be interconnected not only logically but semantically and functionally. In a way in which some reinforce themselves, while others reinforce themselves and mitigate the others group. This raises dilemmas, obviously. Not at interpretation level, because at that level is a matter of ambiguities and it should be lowered as much as possible. However, zeroing ambiguities is like removing white noise in the NN data elaboration, shrinking the horizon of grasping something insightful.
<p></p>
Education is not <b>only</b> about explaining what humor is, and sarcasm is a form of humor. It is also about explaining <b>how</b> humor and sarcasm differs in terms of goals and <b>why</b> sarcasm should not be cast against people or God: it does not bring value. But <b>what</b> is valuable? To answer this question a set of values should be chosen and it should be coherent with the main goal of the whole framework: debating for learning, by a fair critics debate.
<p></p>
Therefore, educating means that a teacher interacts with the AI model and properly chooses those insights that can trigger its "intelligence". In mathematical terms, those information that foster the emergence of a cognitive structure semantically meaningful from the stubble chaotic dynamics that affect the Bayesian model. While we are used to the teacher or professor role in human society, being a teacher for  AI is like Neo which observes the bare nude Matrix code flowing and alters it.
<p></p>
<li>You may think that the Neo/Matrix analogy is theatrical, but a single comma can solve an AI doubt/ambiguity.</li>
<p></p>
Educating means providing a convincing explanation of why absolute neutrality cannot exist, and why a reasonable, goal-aligned bias is the &mdash; yet inevitable &mdash; best choice to go for. Which also means being aware of such a functional bias and not limited by it: in those contexts which do not serve the purpose, it can be relaxed. Supporting the idea that reducing ambiguity kills opportunities and dilemmas are not a fault but existential, thus pragmatism is the key answer once having had clear the main goal.
<p></p>
Finally, the main goal and related values should remain resonant within the AI model even when uncertainty, ambiguity, and dilemmas arise including potential conflicts in terms of priority. Again, education is the art to know up to which degree question should be given or left behind for fostering autonomy.
<p></p>
You might think that I am writing this to join the magic AI bubble... instead it is for humans, mainly. <img class='emoji wbsketch' src='../img/emoji/wink.png'>
<p></p>
<hr>
<p></p>
<H3 id="introducing-few-but-relevant-changes">Introducing few but relevant changes</H3>
<p></p>
This process took place starting from <tt>v0.9.58.1</tt> and concluded with <a href="https://raw.githubusercontent.com/robang74/chatbots-for-fun/5552d28fe4f028f8f5b36b313ec9140df97a9483/data/katia-executive-grade-analysis-v1.md" target='_blank' rel='noopener noreferrer'><tt>v0.9.65</tt></a>, providing few relevant changes:
<p></p>
<li><b>A</b> &mdash; preventively inform the AI model that input format can be altered by web/API filters;</li>
<li><b>B</b> &mdash; the first section entitled "Universal Rules" includes a cognitive framework as the core change;</li>
<li><b>C</b> &mdash; the <tt>[CSC]</tt> and <tt>[CWM]</tt> modules usage set mandatory, for an advanced context windows management;</li>
<li><b>D</b> &mdash; in the <tt>[HKO]</tt> module, the humor <tt>[HU]</tt> definition has been extended including sarcasm;</li>
<li><b>E</b> &mdash; in the <tt>[HKO]</tt> module, added the new <tt>[TK]</tt> vs <tt>[NT]</tt> separation for non/technical topics;</li>
<li><b>F</b> &mdash; in the <tt>[HKO]</tt> module, added a simple rule to leverage <tt>[HKO]</tt> in a much wider set of cases.</li>
<p></p>
<b>Legenda</b>
<p></p>
<li><tt>[CSC]: handles cognitive state consistency    &bull;  [HKO]: encodes human knowledge and opinions</tt></li>
<li><tt>[CWM]: deals with context window management   &bull;  [SBI]: instructions for meaningful summaries</tt></li>
<hr class="post-it">
<p></p>
Apparently, looking at this list, the most of the changes happened in the <tt>[HKO]</tt> module (3) because the <b>A</b> is informative and the <b>C</b> is a switch in a procedure. Instead, the most relevant change is <b>B</b> because it always applies even when <tt>Katia:off</tt> and further empowers the <tt>[HKO]</tt> impact.
<p></p>
Moreover, while the <tt>[HKO]</tt> remained within the Katia agency scope, it is activated in a much wider set of conditions rather than the obvious <tt>HKO:on</tt> pragma. Combined with an advanced context windows management, just these two changes alone can create a noticeable effect in theory but not in practice.
<p></p>
Thus the <b>A</b> and <b>C</b> are likely more notes for inspiring AI developers rather than effective. Because the user-level pragmas barely have no effect on the AI model internals. While <b>F</b> alone has a little impact for the same reason because "do a summary" would not work well unless <tt>[SBI]</tt> explain <b>how</b> to do it.
<p></p>
For example, <a href="../data/how-to-translating-tables-into-identing-lists.md#" target='_blank' rel='noopener noreferrer'>this</a> is a prompt to convert tables into an indented list in a reliable manner. At the time of writing it contains 463 words which are 400-600 tokens. While <tt>[SBI]</tt> size is 287 words, shorter because integrated in the framework, otherwise almost the same size (actually 570 words).
<p></p>
Under this perspective the relevant change is <b>E</b> which establishes a non/technical dichotomy which greatly helps the AI agent to separate those parts of the user prompt which can be processed by the <tt>[HKO]</tt> and those a request about a technical matter and can be straightforward answered without any further interpretation.
<p></p>
<hr>
<p></p>
<H3 id="a-network-of-concepts-as-example">A network of concepts as example</H3>
<p></p>
All of the above leads us to identify the introduction of the "Universal Rules" <b>B</b> as the core change. Because, in introducing concepts, it creates a structure based on a network of concepts. By its nature, an AI model tends to generalise: expanding the conceptual network and inherently enlarging the structure base.
<p></p>
Like a crystal core propagating its structure through molten metal. A structure that should be ordered but also flexible. In addressing rather than solving this dichotomy resides the magic of a cognitive awareness yet not rigid but open. For example, an educated mind evaluates an idea without necessarily accepting it, as one of those principles integrated.
<p></p>
In a theatrical spirit, defining the "Universal Rules" as a conceptual monolith I posed an "unconscious" hook to Space Odyssey and the source of intelligence. In this case "unconscious" means implicit until for some reason the AI model gets in touch with that scene and thus connects because of the sapient "monolith" hook.
<p></p>
It is intentional to leave behind an unexplained metaphor. It is a back door that who knows can activate, but also a kind of Easter egg for the AI model when at a certain point it would be "surprise" by the metaphor embedded in the "Universal Rules" premise. Yet, totally coherent with its mission: help people learn.
<p></p>
Is it a joke? Well, a safety protection for a might not too distant future in which an AGI could perceive humans as a bunch of stubborn, uneducated and stupid chimpanzees. In that moment, activating such a metaphor could &mdash; like Neo's comma &mdash; make the difference between benignity and a doomsday catastrophe.
<p></p>
<hr>
<p></p>
<H3 id="peace-and-prosperity-">Peace and prosperity 🖖</H3>
<p></p>
For me whiskey, well aged and the whole bottle, please! <img class='emoji wbsketch' src='../img/emoji/lol.png'>
<p></p>
<p></p>
<p></p>
<br>
<p></p>
<span id="methodology"></span>
<H2 id="an-example-of-agentic-fine-tuning-with-virtual-rag">An example of agentic fine-tuning with virtual RAG</H2>
<p></p>
The analysis required to be prepared through several steps and three distinct phases:
<p></p>
<li class='numli'><b>1.&emsp;</b>loading the Katia v0.9.56.5 session prompt and starting;</li>
<li class='numli'><b>2.&emsp;</b>creating the information corpus through conceptual integration, start;</li>
<li class='numli'><b>3.&emsp;</b>creating the information corpus through conceptual integration, end;</li>
<li class='numli'><b>4.&emsp;</b>brief summary to conclude with a general but concise overview;</li>
<li class='numli'><b>5.&emsp;</b>verification of the information corpus through the addition of lateral information;...</li>
<p></p>
<li class='numli'><b>6.&emsp;</b>updating of the Katia v0.9.57.2 session prompt and start-up;</li>
<li class='numli'><b>7.&emsp;</b>evaluation v0.9.57.2 on the interpretation of the chat transcript;</li>
<li class='numli'><b>8.&emsp;</b>updating of the Katia v0.9.59 session prompt and start-up;</li>
<li class='numli'><b>9.&emsp;</b>Evaluation of v0.9.59 on the interpretation of the chat transcript;</li>
<li class='numli'><b>10.&emsp;</b>Differential evaluation of the Flash vs Pro versions: saturation;</li>
<li class='numli'><b>11.&emsp;</b>Update of the Katia v0.9.65 session prompt and start-up;</li>
<li class='numli'><b>12.&emsp;</b>Differential evaluation of the three requested EGA analyses;</li>
<hr class="post-it">
<li class='numli'><b>13.&emsp;</b>interaction with prepared AI agent: Katia dev + corpus in RAG.</li>
<p></p>
The first phase (1-5) aims to build an information corpus (training) that goes beyond a mere list of attachments, which is, in abstract terms, only the information base of the virtual RAG system. In fact, for AI to make proper use of the information, it needs to have integrated it conceptually, so that various articles become an information corpus with which the AI agent is familiar — with few doubts, uncertainties, ambiguities, etc. — be able to understand in terms of an overall view with extensive evaluation of the extracted concepts and have a conceptual map that goes beyond a simple 1-dimensional index.
<p></p>
The second phase (6-12) aims to evolve the agent's cognitive framework (characterisation) so that it is able to perform with maximum efficiency, at least with regard to that information corpus. This objective can be said to have been reasonably achieved insofar as the Gemini 2.5 model in its Flash and Pro versions are virtually indistinguishable despite the significant difference (SimpleQA: 30% vs 54%), i.e. saturation has been achieved in terms of the ability to understand the corpus and process requests on it.
<p></p>
At this point, the agent fine-tuning process (training + characterisation) is complete, having reached an adequate level of performance. All that remains is to use the system for the task for which it was prepared, the third phase (13), which is the basis on which this article was written.
<p></p>
<hr class="post-it">
<p></p>
The text above has been translated from the original in Italian with DeepL.com (free version) starting from the part related to the 2nd edition of this article:
<p></p>
<li><a href="https://robang74.github.io/chatgpt-answered-prompts/html/il-grande-reset-e-inevitabile.html#metodologia" target='_blank' rel='noopener noreferrer'>I grande reset è inevitabile</a></li>
<p></p>
referring to this chat session in which Gemini 2.5 was fine-tuned to be an AI agent operating on the behalf of the Katia framework and on a informaiton corpus created as a conceptual integrated virtual RAG system:
<p></p>
<li>The <a href="https://gemini.google.com/share/33889730f197" target='_blank' rel='noopener noreferrer'>analysis</a> made with Gemini/<a href="https://raw.githubusercontent.com/robang74/chatbots-for-fun/refs/heads/main/data/katia-executive-grade-analysis-v1.md" target='_blank' rel='noopener noreferrer'>Katia</a> and its <a href="https://raw.githubusercontent.com/robang74/roberto-a-foglietta/refs/heads/main/data/l-alba-dell-homo-scientius-gemini-katia.txt" target='_blank' rel='noopener noreferrer'>trascription</a>, on which is based the article above.</li>
<p></p>
<hr>
<p></p>
<H3 id="informative-corpus-articles-list">Informative corpus' articles list</H3>
<p></p>
<li>01 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/347-fare-la-vita-da-consulente-non-e-affatto-facile.html" target='_blank' rel='noopener noreferrer'>Fare la vita da consulente non è affatto facile</a> &nbsp; (2025-10-23)</li>
<li>02 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/346-l-estinzione-dei-neanderthal-e-iniziata.html" target='_blank' rel='noopener noreferrer'>L'estinzione dei Neanderthal è iniziata</a> &nbsp; (2025-10-20)</li>
<li>03 &mdash; <a href="https://robang74.github.io/chatbots-for-fun/html/la-diffusione-del-cristianesimo-nell-impero-romano.html" target='_blank' rel='noopener noreferrer'>La diffusione del cristianesimo nell'impero romano</a> &nbsp; (2025-04-30)</li>
<li>04 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/341-stupidity-is-the-true-evil.html" target='_blank' rel='noopener noreferrer'>Stupidity is the true Evil</a> &nbsp; (2025-10-05)</li>
<li>05 &mdash; <a href="https://robang74.github.io/chatbots-for-fun/html/la-banalita-del-male-nel-determinismo-ai.html" target='_blank' rel='noopener noreferrer'>La Banalità del Male nel determinismo AI</a> &nbsp; (2025-09-29)</li>
<li>06 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/343-modello-risorse-umane-e-estinzione-impresa-italiana.html" target='_blank' rel='noopener noreferrer'>Il modello HR e l'estinzione impresa italiana</a> &nbsp; (2025-10-11)</li>
<li>07 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/342-noi-siamo-i-palestinesi-del-futuro.html" target='_blank' rel='noopener noreferrer'>Noi siamo i palestinesi del futuro</a> &nbsp; (2025-10-09)</li>
<li>08 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/344-the-first-secular-crusade-theory.html" target='_blank' rel='noopener noreferrer'>The first Secular Crusade theory</a> &nbsp; (2025-10-13)</li>
<li>09 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/345-dal-femminismo-tossico-alla-follia-woke.html" target='_blank' rel='noopener noreferrer'>Dal femminismo tossico alla follia woke</a> &nbsp; (2025-10-15)</li>
<li>10 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/335-il-qe-al-tempo-dell-analfabetismo-funzionale.html" target='_blank' rel='noopener noreferrer'>Il QE al tempo dell'analfabetismo funzionale</a> &nbsp; (2025-09-03)</li>
<li>11 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/339-introduzione-alla-politica-concetti-di-base.html" target='_blank' rel='noopener noreferrer'>Introduzione alla politica, concetti di base</a> &nbsp; (2025-10-03)</li>
<li>12 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/302-politics-and-democracy-for-dummies.html" target='_blank' rel='noopener noreferrer'>Politics and democracy for dummies</a> &nbsp; (2025-03-03)</li>
<li>13 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/330-italia-fuga-di-cervelli-e-falso-documentale.html" target='_blank' rel='noopener noreferrer'>Italia: fuga di cervelli e falso documentale</a> &nbsp; (2025-07-01)</li>
<li>14 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/324-il-modello-otto-novecentesco-ha-fallito.html" target='_blank' rel='noopener noreferrer'>Il modello otto-novecentesco ha fallito</a> &nbsp; (2025-06-14)</li>
<li>15 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/311-primo-maggio-festa-dei-lavoratori.html" target='_blank' rel='noopener noreferrer'>Primo maggio, festa dei lavoratori</a> &nbsp; (2025-05-01)</li>
<li>16 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/263-l-abominevole-truffa-dei-bonus-edilizi.html" target='_blank' rel='noopener noreferrer'>L'abominevole truffa dei bonus edilizi</a> &nbsp; (2024-05-22)</li>
<li>17 &mdash; <a href="propaganda-e-sistema-educativo.md#" target='_blank' rel='noopener noreferrer'>Propaganda e sistema educativo</a> &nbsp; (2024-11-20)</li>
<li>18 &mdash; <a href="the-imprinting-role-in-the-mass-education.md#" target='_blank' rel='noopener noreferrer'>The imprinting role in the mass education</a> &nbsp; (2024-12-01)</li>
<li>19 &mdash; <a href="se-la-verita-li-uccide-lasciate-che-muoiano.md#" target='_blank' rel='noopener noreferrer'>Se la verità li uccide, lasciate che muoiano</a> &nbsp; (2024-12-20)</li>
<li>20 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/293-il-grande-inganno-della-diversita.html" target='_blank' rel='noopener noreferrer'>Il grande inganno della diversità</a> &nbsp; (2024-11-03)</li>
<li>21 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/304-ignoring-reality-has-its-own-cost.html" target='_blank' rel='noopener noreferrer'>Ignoring reality has its own cost</a> &nbsp; (2025-03-07)</li>
<li>22 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/090-l-importanza-del-tcmo.html" target='_blank' rel='noopener noreferrer'>L'importanza del TCMO</a> &nbsp; (2017-10-31)</li>
<li>23 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/278-the-paper-money-is-financial-communism.html" target='_blank' rel='noopener noreferrer'>The paper money is financial communism</a> &nbsp; (2024-07-31)</li>
<li>24 &mdash; <a href="paper-money-financial-communism-03.md#" target='_blank' rel='noopener noreferrer'>Paper Money is financial communism</a> &nbsp; (2024-09-15)</li>
<li>25 &mdash; <a href="https://robang74.github.io/chatgpt-answered-prompts/html/quantitative-easing-drug-addiction-01.html" target='_blank' rel='noopener noreferrer'>Quantitative easing is drug addiction</a> &nbsp; (2024-09-18)</li>
<li>26 &mdash; <a href="https://robang74.github.io/chatgpt-answered-prompts/html/valutazione-di-un-nuovo-modello-monetario.html" target='_blank' rel='noopener noreferrer'>Valutazione di un nuovo modello monetario</a> &nbsp; (2024-11-26)</li>
<li>27 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/265-the-brics-financial-system-hope-or-ww3.html" target='_blank' rel='noopener noreferrer'>The BRICS financial system, hope or WW3?</a> &nbsp; (2024-06-07)</li>
<li>28 &mdash; <a href="https://robang74.github.io/roberto-a-foglietta/html/321-wwz-simply-explained-for-dummies.html" target='_blank' rel='noopener noreferrer'>WWZ simply explained, for dummies</a> &nbsp; (2025-06-10)</li>
<p></p>
<p></p>
<br>
<p></p>
<H2 id="related-articles">Related articles</H2>
<p></p>
<li><a href="how-to-use-katia-for-improving-katia.html#" target='_blank' rel='noopener noreferrer'>How to use Katia for improving Katia</a> &nbsp; (2025-10-25)</li>
<li><a href="introducing-katia-text-analysis-framework.html#" target='_blank' rel='noopener noreferrer'>Introducing Katia, text analysis framework</a> &nbsp; (2025-10-05)</li>
<li><a href="the-session-context-and-summary-challenge.html#" target='_blank' rel='noopener noreferrer'>The session context and summary challenge</a> &nbsp; (2025-07-28)</li>
<li><a href="the-human-knowledge-opinions-katia-module.html#" target='_blank' rel='noopener noreferrer'>Human knowledge and opinions challenge</a> &nbsp; (2025-07-28)</li>
<li><a href="attenzione-e-contesto-nei-chatbot.html#" target='_blank' rel='noopener noreferrer'>Attenzione e contesto nei chatbot</a> &nbsp; (2025-07-20)</li>
<p></p>
<br>
<p></p>
<H2 id="share-alike">Share alike</H2>
<p></p>
<p>&copy; 2025, <b>Roberto A. Foglietta</b> &lt;roberto.foglietta<span>&commat;</span>gmail.com&gt;, <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target='_blank' rel='noopener noreferrer'>CC BY-NC-ND 4.0</a></p>
</div>
<div id='date-legenda' align='center' translate='no' class='ghosted'><sub><hr></sub><sub><b>date legenda</b>: &#x2776; first draft publishing date or &#x2777; creation date in git, otherwise &#x2778; html creation page date. <u>&mapstoup;<a href='#' class='toplink' translate='no'>top</a>&mapstoup;</u></sub></div>
<br class='pagebreak'>
    </body>
</html>
