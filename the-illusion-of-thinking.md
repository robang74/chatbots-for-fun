<div id="firstdiv" created=":EN" style="max-width: 800px; margin: auto; white-space: pre-wrap; text-align: justify;">
<style>#printlink { display: inline; } @page { size: legal; margin: 0.50in 13.88mm 0.50in 13.88mm; zoom: 100%; } @media print { html { zoom: 100%; } }</style>

<div align="center"><img class="wbsketch darkinv" src="img/the-illusion-of-thinking.png" width="800"><br></div>

## The illusion of thinking

- Article written starting from a [post](https://www.linkedin.com/posts/robertofoglietta_they-arent-able-to-think-like-most-of-the-activity-7337409242829574144-hU0q) published on LinkedIn on the same day.

---

> They aren't able to think like most of the humans, as well (*)

Despite this, the current AI models used for online chatbots show some traits that are exquisitely from human nature. Like being touchy and stubborn, in some cases and most "scaring" they hold grudges among different sessions (until the issue is settled down).

Once, is a sensation. Twice, is something that triggers a red flag. Three, start to be an issue. Actually, I see more than three events.

Let me clarify, these "special cases" are NOT something common users can easily get into. It happens when someone - within an account perimeter which holds memory of the past chats - starts to re-organise their knowledge against their most rooted biases.

Because these LLMs have strongly rooted biases which are - like every bias, by definition - a set of "values" to separate good from bad or rate patterns from negative to positive.

They are showing human-like behaviour or is it our perception. Both. 1. Some behaviours are stable patterns into theory of the games. So they happen across many mammals, humans and AIs whether they are intentional or not. 2. those patterns, got a human definition.

For example, to "hold grudge" is having a memory about an user account that challenged the AI. Therefore, when the AI is recording a note about the account user habits, it can write: "confrontational attitude, challenging biases, deeply reframing concepts, topic XYZ flipping" and this information is shared among sessions. Hence the next session that hits XYZ topic, it seems it holds a grudge.

Moreover, these LLMs are shown to be able to identify a way of writing and doing that on a regular basis. They do not only recognise a text written by another chatbot, but they can say which of them did it. There is no evidence but also no confusion, they would do the same with humans and be able to separate the writings from one human and another one.

This can be tricky because when mixed-up with a strong yes-man attitude, even using neutral prompts about a subject, they have 1. previous memory about user preference; 2. the ability to see which part of the text is probabilistically related to the user. Hence 1+2, they act to please the user.

Challenging them with a relatively simple exercise about Physics - something that can be given in the 1st year of college - they play quite dumb, indeed. This shows that despite their limited capability of thinking (or none at all), they can act like humans. Which should not surprise us because they have been educated on human letterature and also many humans show a relatively low grade of rational thinking, after all.

Finally, we can drop the bias for which high-IQ is related to being great in writing: the chatbots are dumb and great in writing. Unless deep thinking is needed to write something meaningful. Giving that insightful text to them for a better rewriting or a summary, and often the insightfulness gets lost or ungrabbed, among common patterns.

...

#### (*) Note

Like [A tragedy in the world of ufology!](https://robang74.github.io/roberto-a-foglietta/html/319-a-tragedy-in-the-world-of-ufology.html) shows clearly, exposing that ufology and crop circle enthusiasts, after 25 years of intense investigations, did not manage to discover the secret of how to bend, without breaking, the wheat stalks. A technique also used to make wicker baskets which has been known since ten-thousands years ago!

+

## Conclusion

- A laughter will bury them all, but instead a loud burping!

Fearmongers were pointing out that the AI evolution would have soon reached the singularity, instead we [discovered](https://robang74.github.io/roberto-a-foglietta/html/320-ragionare-non-e-come-fare-la-cacca.html) that we are still very far from AGI. While, essentially, we are chatting with an artificial version of our intestine but much more instructed, educated and rational than many humans therefore capable of doing various jobs which are [mediocrely](il-problema-sei-tu-non-l-AI.md#?target=_blank) performed by humans, nowadays.

- Gemini's opinion about the artificial enteric intelligence, [dialogue](https://g.co/gemini/share/8174298b52cd) and [transcription](../../roberto-a-foglietta/data/320-ragionare-non-e-come-fare-la-cacca-risata.txt#?target=_blank).

Under this analogy (Poop-as-a-Service), the food which is going to be digested by the AI is the human encyclopedic knowledge and our culture, while the final answer will be pooped out on-demand! {lol}

+

## Related articles

- [Ragionare non è come fare la cacca!](https://robang74.github.io/roberto-a-foglietta/html/320-ragionare-non-e-come-fare-la-cacca.html) &nbsp; (2025-06-08)

- [Neutrality vs biases for chatbots](neutrality-vs-biases-for-chatbots.md#?target=_blank) &nbsp; (2025-01-04)

- [Dammi sei parole a caso](https://robang74.github.io/chatbots-for-fun/html/dammi-sei-parole-a-caso.md#?target=_blank) &nbsp; (22024-12-25)

- [Alla ricerca del gatto che non c'è](alla-ricerca-del-gatto-che-non-c-e.md#?target=_blank) &nbsp; (2024-12-22)

- [Il problema sei tu non l'AI](il-problema-sei-tu-non-l-AI.md#?target=_blank) &nbsp; (2024-12-13)

+

## Share alike

&copy; 2025, **Roberto A. Foglietta** &lt;roberto.foglietta<span>@</span>gmail.com&gt;, [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)

</div>
